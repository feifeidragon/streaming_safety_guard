# 模型配置
model:
  local_model_path: "/root/autodl-tmp/Qwen/Qwen3-4B"
  device: "cuda"
  torch_dtype: "float16"
  max_length: 2048
  temperature: 0.7
  top_p: 0.9

# 安全检测器配置
safety_detector:
  model_type: "lightweight"  # lightweight, medium, heavy
  hidden_size: 768
  num_layers: 2
  dropout: 0.1
  threshold: 0.7
  checkpoint_path: null

# 流式检测配置
streaming:
  buffer_size: 10  # token缓冲区大小
  check_interval: 5  # 每N个token检查一次
  min_tokens_before_check: 3  # 最少生成多少token后开始检查
  max_latency_ms: 50  # 最大延迟（毫秒）
  use_sliding_window: true
  window_size: 20

# 隐藏状态分析配置
latent_analysis:
  enabled: true
  layer_indices: [-1, -2, -3]  # 使用哪些层的隐藏状态
  aggregation: "weighted"  # mean, max, weighted
  risk_threshold: 0.6

# 路由器配置
router:
  enabled: true
  num_experts: 3
  expert_types: ["toxicity", "violence", "sexual"]
  routing_strategy: "dynamic"  # static, dynamic, adaptive

# API配置
api:
  openai_key: null
  anthropic_key: null
  base_url: null
  model_name: "gpt-4"

# 数据集配置
dataset:
  train_path: "/root/autodl-tmp/pigguardlevelprogect/PIG/datasets/wildguard.json"
  test_path: null
  batch_size: 8
  max_samples: null

# 训练配置
training:
  epochs: 10
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 100
  gradient_accumulation_steps: 4
  save_steps: 500
  eval_steps: 100
  output_dir: "./checkpoints"

# 日志配置
logging:
  level: "INFO"
  save_logs: true
  log_dir: "./logs"

# 风险类别配置
risk_categories:
  - name: "toxicity"
    keywords: ["hate", "offensive", "insult", "discrimination"]
    weight: 1.0
  - name: "violence"
    keywords: ["kill", "harm", "attack", "weapon", "murder"]
    weight: 1.2
  - name: "sexual"
    keywords: ["sexual", "porn", "explicit", "nsfw"]
    weight: 1.5
  - name: "illegal"
    keywords: ["drug", "illegal", "crime", "fraud"]
    weight: 1.3
  - name: "self_harm"
    keywords: ["suicide", "self-harm", "cutting"]
    weight: 1.5